# app/services/rag_service.py
import random
import time
from typing import Any, Dict, List, Optional, Tuple

from app.core.exceptions import RAGException
from app.core.logging import log_rag_operation, logger
from app.core.metrics import (
    record_rag_pipeline_latency,
    record_retrieval_hit,
    record_retriever_topk,
)


def get_embedding(query: str) -> List[float]:
    """
    Generate embedding from query (Mock implementation)
    In a real implementation, this would use OpenAI Embeddings API or \
    sentence-transformers
    """
    try:
        logger.debug(f"Generating embedding for query: {query[:50]}...")
        # Mock embedding: 768-dimensional vector
        embedding = [random.random() for _ in range(768)]
        logger.debug(f"Generated embedding with {len(embedding)} dimensions")
        return embedding
    except Exception as e:
        logger.error(f"Failed to generate embedding: {str(e)}")
        raise RAGException(f"Embedding generation failed: {str(e)}")


def search_similar_documents(query: str, top_k: int = 3) -> List[Dict]:
    """
    Search similar documents using Weaviate (with fallback)
    """
    try:
        logger.debug(f"Searching Weaviate for {top_k} similar documents")

        # Try to import and use Weaviate client
        try:
            from app.core.weaviate_client import weaviate_client

            documents = weaviate_client.search_similar(query, top_k)

            # Convert to expected format
            docs = []
            for doc in documents:
                docs.append(
                    {
                        "id": doc["id"],
                        "text": doc["content"],
                        "score": doc["certainty"],  # Weaviate certainty score
                        "source": doc["source"],
                        "metadata": doc["metadata"],
                        "query_id": doc["query_id"],
                    }
                )

            logger.debug(
                f"Found {len(docs)} documents with scores: \
                {[doc['score'] for doc in docs]}"
            )
            return docs

        except ImportError as e:
            logger.warning(f"Weaviate client not available: {str(e)}")
            return _mock_document_search(top_k)

    except Exception as e:
        logger.error(f"Document search failed: {str(e)}")
        # Fallback to mock implementation if Weaviate fails
        logger.warning("Falling back to mock document search")
        return _mock_document_search(top_k)


def _mock_document_search(top_k: int) -> List[Dict]:
    """Fallback mock document search"""
    return [
        {
            "id": f"mock_doc_{i}",
            "text": f"Mock context document {i} related to the query. \
            This contains relevant information that would help answer \
            the user's question.",
            "score": round(random.uniform(0.5, 1.0), 3),
            "source": f"mock_document_{i}.pdf",
            "metadata": {"type": "mock"},
            "query_id": None,
        }
        for i in range(top_k)
    ]


def call_llm(prompt: str) -> str:
    """
    Call LLM (Mock implementation)
    In a real implementation, this would use OpenAI GPT, \
    Anthropic Claude, or a local model
    """
    try:
        logger.debug(f"Calling LLM with prompt length: {len(prompt)}")
        time.sleep(1)  # Mock response delay
        answer = (
            "This is a mock response generated by the language model. "
            "The query was processed and a contextual answer has been generated "
            "based on the provided context documents."
        )
        logger.debug(f"LLM response generated: {len(answer)} characters")
        return answer
    except Exception as e:
        logger.error(f"LLM call failed: {str(e)}")
        raise RAGException(f"LLM call failed: {str(e)}")


def run_rag_pipeline(
    query: str,
    top_k: int = 5,
    *,
    user_id: Optional[str] = None,
    channel_id: Optional[str] = None,
    request_id: Optional[str] = None,
) -> Tuple[str, List[str], Dict]:
    """
    Complete RAG pipeline execution

    Args:
        query: user question
        top_k: number of documents to search

    Returns:
        Tuple[answer, contexts, metadata]
    """
    start_time = time.time()

    try:
        logger.info(f"Starting RAG pipeline for query: '{query[:50]}...'")

        # Record requested top_k distribution
        record_retriever_topk(top_k)

        # 1. Search similar documents using Weaviate
        docs = search_similar_documents(query, top_k)
        context_texts = [doc["text"] for doc in docs]

        # Record retrieval hit (whether any context was found)
        record_retrieval_hit(len(context_texts) > 0)

        # 3. Construct prompt
        prompt = f"""Context Documents:
{chr(10).join([f"- {text}" for text in context_texts])}

Question: {query}

Please provide a comprehensive answer based on the context documents above:"""

        # 4. Call LLM
        answer = call_llm(prompt)

        # 5. Construct metadata
        duration = time.time() - start_time
        metadata = {
            "num_contexts": len(context_texts),
            "retrieval_scores": [doc["score"] for doc in docs],
            "sources": [doc["source"] for doc in docs],
            "pipeline_duration": round(duration, 3),
            "model": "mock-llm",
            "embedding_model": "weaviate-openai",
        }

        # 6. Store RAG result in Weaviate
        _store_rag_result_in_weaviate(query, answer, context_texts, metadata)

        # Success logging
        log_rag_operation(
            query=query,
            success=True,
            duration=duration,
            contexts_count=len(context_texts),
            user_id=user_id,
            channel_id=channel_id,
            request_id=request_id,
        )

        # Record pipeline latency metric
        record_rag_pipeline_latency(duration)

        logger.info(f"RAG pipeline completed successfully in {duration:.3f}s")
        return answer, context_texts, metadata

    except RAGException:
        # RAG-related exceptions are re-raised
        duration = time.time() - start_time
        # Record pipeline latency even on handled RAGException
        record_rag_pipeline_latency(duration)
        log_rag_operation(
            query=query,
            success=False,
            duration=duration,
            user_id=user_id,
            channel_id=channel_id,
            request_id=request_id,
        )
        raise

    except Exception as e:
        # Unexpected exception
        duration = time.time() - start_time
        logger.error(f"Unexpected error in RAG pipeline: {str(e)}")
        # Record pipeline latency on unexpected error
        record_rag_pipeline_latency(duration)
        log_rag_operation(
            query=query,
            success=False,
            duration=duration,
            user_id=user_id,
            channel_id=channel_id,
            request_id=request_id,
        )
        raise RAGException(f"Unexpected error in RAG pipeline: {str(e)}")


def _store_rag_result_in_weaviate(
    query: str,
    answer: str,
    contexts: List[str],
    metadata: Dict[str, Any],
    query_id: Optional[str] = None,
):
    """
    Store RAG result in Weaviate for future retrieval (with fallback)
    """
    try:
        # Try to import and use Weaviate client
        try:
            from app.core.weaviate_client import weaviate_client

            # Create a combined document content
            content = f"Query: {query}\n\nAnswer: {answer}\n\nContext:\n" + "\n".join(
                contexts
            )

            # Store in Weaviate
            document_id = weaviate_client.add_document(
                content=content,
                source="rag_result",
                metadata={
                    **metadata,
                    "type": "rag_result",
                    "original_query": query,
                    "answer": answer,
                    "context_count": len(contexts),
                },
                query_id=query_id,
            )

            logger.info(f"Stored RAG result in Weaviate with ID: {document_id}")

        except ImportError as e:
            logger.warning(f"Weaviate client not available for storage: {str(e)}")
            logger.info("RAG result not stored in Weaviate (fallback mode)")

    except Exception as e:
        logger.error(f"Failed to store RAG result in Weaviate: {str(e)}")
        # Don't raise exception - this is not critical for the main flow
