{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b9f900",
   "metadata": {},
   "source": [
    "# Retrieval Sweep Results Analysis\n",
    "This notebook loads CSV results generated by `scripts/sweep_retrieval.py`,\n",
    "compares performance by configuration, and selects top combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "\n",
    "results_dir = Path('/mnt/data/results')\n",
    "summary_csv = list(results_dir.glob('*.csv'))\n",
    "summary_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59909d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Specify file paths directly\n",
    "summary_path = Path('/mnt/data/results/sweep_run.csv')\n",
    "per_query_path = Path('/mnt/data/results/sweep_run_per_query.csv')\n",
    "\n",
    "summary = pd.read_csv(summary_path)\n",
    "per_query = pd.read_csv(per_query_path)\n",
    "\n",
    "summary.head(), per_query.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14707754",
   "metadata": {},
   "source": [
    "## Top-N Combination Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple score: hit_rate 0.6, kw_hit_rate 0.3, - avg_latency_s 0.1 weighted sum\n",
    "summary = summary.copy()\n",
    "summary['score'] = 0.6*summary['hit_rate'] + 0.3*summary['kw_hit_rate'] - 0.1*summary['avg_latency_s']\n",
    "top = summary.sort_values('score', ascending=False).head(10)\n",
    "top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c362b",
   "metadata": {},
   "source": [
    "## Visualization (matplotlib, following unspecified color rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(summary['avg_latency_s'], summary['hit_rate'])\n",
    "plt.xlabel('Avg Latency (s)')\n",
    "plt.ylabel('Hit Rate')\n",
    "plt.title('Latency vs Hit Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc499c3a",
   "metadata": {},
   "source": [
    "## Performance by Intent/Query Type (Optional: when label column is added to per_query CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: If per_query['intent'] exists, you can see group averages.\n",
    "if 'intent' in per_query.columns:\n",
    "    print(per_query.groupby('intent')[['hit','kw_hit','coverage','latency_s']].mean())\n",
    "else:\n",
    "    print('No intent column in per_query.csv. If you want this, add intent to eval_set and pass it down in the script.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
